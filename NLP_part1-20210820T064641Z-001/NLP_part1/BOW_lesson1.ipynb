{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I love love Programming.\", \" I love Math.\", \" I tolerate Biology.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class __sklearn.feature_extraction.text.CountVectorizer__(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>)[source]¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer has buildin lowercasing, punctuation & stop words removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect = CountVectorizer()\n",
    "text_cvect = cvect.fit_transform(texts)\n",
    "cvect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in CSR format, memory effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cvect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cvect.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try \"binary=True\". E.g. create one-hot encoding representation. This is useful for discrete probabilistic models that model binary events rather than integer counts. May be good option for sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect = CountVectorizer(binary=True)\n",
    "text_cvect = cvect.fit_transform(texts)\n",
    "text_cvect.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets have unigrams &  bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect = CountVectorizer(ngram_range=(1, 2))\n",
    "text_cvect = cvect.fit_transform(texts)\n",
    "cvect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cvect.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes models can be created based on chars. It allows to capture the most common words parts. No need for stemming/lemmatizaion. \n",
    "We can setup max & min number of tokens occurencies with max_df, min_df, max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvect = CountVectorizer(ngram_range=(3, 4), analyzer=\"char\", max_features=10)\n",
    "cvect = CountVectorizer(ngram_range=(3, 4), analyzer=\"char\")\n",
    "text_cvect = cvect.fit(texts)\n",
    "cvect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "class __sklearn.feature_extraction.text.TfidfVectorizer__(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=’word’, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.float64’>, norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(smooth_idf=False, use_idf=False, norm='l1')\n",
    "text_tfidf = tfidf.fit_transform(texts)\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a result of term frequency calculation. No idf applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word \"love\" has lower weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(smooth_idf=False, use_idf=True, norm='l1')\n",
    "text_tfidf = tfidf.fit_transform(texts)\n",
    "text_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "text_tfidf = tfidf.fit_transform(texts)\n",
    "text_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer\n",
    "\n",
    "__sklearn.feature_extraction.text.HashingVectorizer__(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, n_features=1048576, binary=False, norm=’l2’, alternate_sign=True, dtype=<class ‘numpy.float64’>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n",
    "\n",
    "This strategy has several advantages:\n",
    "\n",
    " * it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    " * it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
    " * it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    " \n",
    "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "\n",
    " * there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when  trying to introspect which features are most important to a model.\n",
    " * there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n",
    " * no IDF weighting as this would render the transformer stateful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashv = HashingVectorizer(n_features=20)\n",
    "text_hashv = hashv.fit_transform(texts)\n",
    "text_hashv.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurencies matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the right implementation\n",
    "def co_occurrence(sentences, window_size):\n",
    "    d = defaultdict(int)\n",
    "    vocab = set()\n",
    "    for text in sentences:\n",
    "        # preprocessing (use tokenizer instead)\n",
    "        text = text.lower().split()\n",
    "        text = [w.replace(\".\", \"\") for w in text]\n",
    "        # iterate over sentences\n",
    "        for i in range(len(text)):\n",
    "            token = text[i]\n",
    "            vocab.add(token)  # add to vocab\n",
    "            next_token = text[i+1 : i+1+window_size]\n",
    "            for t in next_token:\n",
    "                key = tuple( sorted([t, token]) )\n",
    "                d[key] += 1\n",
    "        # formulate the dictionary into dataframe\n",
    "    vocab = sorted(vocab) \n",
    "    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),\n",
    "                      index=vocab,\n",
    "                      columns=vocab)\n",
    "    for key, value in d.items():\n",
    "        df.at[key[0], key[1]] = value\n",
    "        df.at[key[1], key[0]] = value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence(texts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
