{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I love love Programming.\", \" I love Math.\", \" I tolerate Biology.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class __sklearn.feature_extraction.text.CountVectorizer__(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>)[source]¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer has buildin lowercasing, punctuation & stop words removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 1, 'programming': 3, 'math': 2, 'tolerate': 4, 'biology': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect = CountVectorizer()\n",
    "text_cvect = cvect.fit_transform(texts)\n",
    "cvect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in CSR format, memory effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cvect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 2, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cvect.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try \"binary=True\". E.g. create one-hot encoding representation. This is useful for discrete probabilistic models that model binary events rather than integer counts. May be good option for sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect = CountVectorizer(binary=True)\n",
    "text_cvect = cvect.fit_transform(texts)\n",
    "text_cvect.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets have unigrams &  bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 1,\n",
       " 'programming': 6,\n",
       " 'love love': 2,\n",
       " 'love programming': 4,\n",
       " 'math': 5,\n",
       " 'love math': 3,\n",
       " 'tolerate': 7,\n",
       " 'biology': 0,\n",
       " 'tolerate biology': 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect = CountVectorizer(ngram_range=(1, 2))\n",
    "text_cvect = cvect.fit_transform(texts)\n",
    "cvect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 2, 1, 0, 1, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cvect.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes models can be created based on chars. It allows to capture the most common words parts. No need for stemming/lemmatizaion. \n",
    "We can setup max & min number of tokens occurencies with max_df, min_df, max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i l': 3,\n",
       " ' lo': 1,\n",
       " 'lov': 5,\n",
       " 'ove': 7,\n",
       " 've ': 9,\n",
       " 'i lo': 4,\n",
       " ' lov': 2,\n",
       " 'love': 6,\n",
       " 'ove ': 8,\n",
       " ' i ': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect = CountVectorizer(ngram_range=(3, 4), analyzer=\"char\", max_features=10)\n",
    "#cvect = CountVectorizer(ngram_range=(3, 4), analyzer=\"char\")\n",
    "text_cvect = cvect.fit(texts)\n",
    "cvect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "class __sklearn.feature_extraction.text.TfidfVectorizer__(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=’word’, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.float64’>, norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 1, 'programming': 3, 'math': 2, 'tolerate': 4, 'biology': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(smooth_idf=False, use_idf=False, norm='l1')\n",
    "text_tfidf = tfidf.fit_transform(texts)\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a result of term frequency calculation. No idf applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.66666667, 0.        , 0.33333333, 0.        ],\n",
       "        [0.        , 0.5       , 0.5       , 0.        , 0.        ],\n",
       "        [0.5       , 0.        , 0.        , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word \"love\" has lower weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.57254423, 0.        , 0.42745577, 0.        ],\n",
       "        [0.        , 0.4010942 , 0.5989058 , 0.        , 0.        ],\n",
       "        [0.5       , 0.        , 0.        , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(smooth_idf=False, use_idf=True, norm='l1')\n",
    "text_tfidf = tfidf.fit_transform(texts)\n",
    "text_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.65985664, 0.43381609, 0.        , 0.43381609,\n",
       "         0.        , 0.43381609, 0.        , 0.        ],\n",
       "        [0.        , 0.4736296 , 0.        , 0.62276601, 0.        ,\n",
       "         0.62276601, 0.        , 0.        , 0.        ],\n",
       "        [0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.57735027, 0.57735027]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "text_tfidf = tfidf.fit_transform(texts)\n",
    "text_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer\n",
    "\n",
    "__sklearn.feature_extraction.text.HashingVectorizer__(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, n_features=1048576, binary=False, norm=’l2’, alternate_sign=True, dtype=<class ‘numpy.float64’>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n",
    "\n",
    "This strategy has several advantages:\n",
    "\n",
    " * it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    " * it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
    " * it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    " \n",
    "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "\n",
    " * there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when  trying to introspect which features are most important to a model.\n",
    " * there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n",
    " * no IDF weighting as this would render the transformer stateful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I love love Programming.\", \" I love Math.\", \" I tolerate Biology.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        , -0.66666667,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.33333333,  0.        ,  0.        ],\n",
       "        [ 0.        , -0.5       ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.5       ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.5       ,  0.5       ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashv = HashingVectorizer(n_features=10, norm=\"l1\")\n",
    "text_hashv = hashv.fit_transform(texts)\n",
    "text_hashv.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurencies matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the right implementation\n",
    "def co_occurrence(sentences, window_size):\n",
    "    d = defaultdict(int)\n",
    "    vocab = set()\n",
    "    for text in sentences:\n",
    "        # preprocessing (use tokenizer instead)\n",
    "        text = text.lower().split()\n",
    "        text = [w.replace(\".\", \"\") for w in text]\n",
    "        # iterate over sentences\n",
    "        for i in range(len(text)):\n",
    "            token = text[i]\n",
    "            vocab.add(token)  # add to vocab\n",
    "            next_token = text[i+1 : i+1+window_size]\n",
    "            for t in next_token:\n",
    "                key = tuple( sorted([t, token]) )\n",
    "                d[key] += 1\n",
    "        # formulate the dictionary into dataframe\n",
    "    vocab = sorted(vocab) \n",
    "    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),\n",
    "                      index=vocab,\n",
    "                      columns=vocab)\n",
    "    for key, value in d.items():\n",
    "        df.at[key[0], key[1]] = value\n",
    "        df.at[key[1], key[0]] = value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biology</th>\n",
       "      <th>i</th>\n",
       "      <th>love</th>\n",
       "      <th>math</th>\n",
       "      <th>programming</th>\n",
       "      <th>tolerate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>biology</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolerate</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             biology  i  love  math  programming  tolerate\n",
       "biology            0  0     0     0            0         1\n",
       "i                  0  0     2     0            0         1\n",
       "love               0  2     1     1            1         0\n",
       "math               0  0     1     0            0         0\n",
       "programming        0  0     1     0            0         0\n",
       "tolerate           1  1     0     0            0         0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occurrence(texts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
